{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM/XGBoost Binary Classification - Python Version\n",
    "\n",
    "This notebook converts the R-based GCW_playcode notebook to Python, demonstrating:\n",
    "- Data simulation and preprocessing\n",
    "- Logistic regression with polynomial features\n",
    "- Gradient Boosting and XGBoost models\n",
    "- Model evaluation and comparison\n",
    "- Hyperparameter tuning\n",
    "\n",
    "Original R notebook: GCW_playcode(2).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if needed (uncomment as necessary)\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(31301)\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avse_plot_classification(data, var, actual, pred, weight, rt_text, lt_text, title=\"Actual vs Predicted\"):\n",
    "    \"\"\"\n",
    "    Python equivalent of R's avse.plot.classification function\n",
    "    Creates actual vs expected plot with exposure bars\n",
    "    \"\"\"\n",
    "    # Group by variable and calculate metrics\n",
    "    grouped = data.groupby(var).agg({\n",
    "        actual: 'sum',\n",
    "        pred: 'sum', \n",
    "        weight: 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    grouped['actual_rate'] = grouped[actual] / grouped[weight]\n",
    "    grouped['predicted_rate'] = grouped[pred] / grouped[weight]\n",
    "    \n",
    "    # Calculate off-balance adjustment\n",
    "    off_balance = data[actual].sum() / data[pred].sum()\n",
    "    grouped['predicted_rate'] = grouped['predicted_rate'] * off_balance\n",
    "    \n",
    "    # Create plot with dual y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Bar plot for exposures\n",
    "    ax1.bar(grouped[var], grouped[weight], alpha=0.3, color='lightblue', label='Exposures')\n",
    "    ax1.set_xlabel(var)\n",
    "    ax1.set_ylabel(lt_text, color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    # Line plots for actual and predicted\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(grouped[var], grouped['actual_rate'], 'o-', color='darkblue', linewidth=3, label='Actual')\n",
    "    ax2.plot(grouped[var], grouped['predicted_rate'], 'o-', color='green', linewidth=3, label='Predicted')\n",
    "    ax2.set_ylabel(rt_text, color='black')\n",
    "    ax2.tick_params(axis='y', labelcolor='black')\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    # Add legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def build_statline(y_train, y_train_pred, y_test, y_test_pred, model_name):\n",
    "    \"\"\"\n",
    "    Build statistics line for model comparison\n",
    "    \"\"\"\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    train_gini = 2 * train_auc - 1\n",
    "    test_gini = 2 * test_auc - 1\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'model_name': [model_name],\n",
    "        'train_auc': [train_auc],\n",
    "        'train_gini': [train_gini], \n",
    "        'test_auc': [test_auc],\n",
    "        'test_gini': [test_gini]\n",
    "    })\n",
    "\n",
    "print(\"Functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Our First GBM (today)\n",
    "\n",
    "## Simulate Some Data\n",
    "\n",
    "First, we will create a dataset to model on. For our purposes, we'll borrow qualitatively our belief about risk differences by age.\n",
    "\n",
    "- Set x_Age to be between 16 and 90\n",
    "- Set probability of event to be some form of cosine function\n",
    "- Use the probabilities to generate binary outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "N = 200000\n",
    "x_Age = np.random.randint(16, 91, N)\n",
    "exposures = np.ones(N)\n",
    "y_truth = (1 + 0.25 * np.cos((x_Age - 15) * np.pi / 45)) * 0.2\n",
    "y_out = np.random.binomial(1, y_truth)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x_Age': x_Age,\n",
    "    'exposures': exposures,\n",
    "    'y_truth': y_truth,\n",
    "    'y_out': y_out\n",
    "})\n",
    "\n",
    "print(f\"Dataset created with {len(df)} observations\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "df_train, df_test = train_test_split(df, test_size=0.5, random_state=31301)\n",
    "\n",
    "# Prepare features and target\n",
    "X_train = df_train[['x_Age']]\n",
    "y_train = df_train['y_out']\n",
    "X_test = df_test[['x_Age']]\n",
    "y_test = df_test['y_out']\n",
    "\n",
    "print(f\"Training set size: {len(df_train)}\")\n",
    "print(f\"Test set size: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data in place, let's take just a little time to visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by age for cleaner visualization\n",
    "df2 = df_train.groupby('x_Age').agg({\n",
    "    'y_truth': 'mean',\n",
    "    'y_out': 'mean',\n",
    "    'exposures': 'count'\n",
    "}).reset_index()\n",
    "df2.columns = ['x_Age', 'f_true', 'f_actual', 'reccnt']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df2['x_Age'], df2['f_true'], 'o-', color='red', label='True')\n",
    "plt.plot(df2['x_Age'], df2['f_actual'], 'o-', color='darkgreen', label='Actual')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('True vs Actual Frequency by Age')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the plotting function\n",
    "avse_plot_classification(df_train, 'x_Age', 'y_out', 'y_out', 'exposures', 'exposures', 'freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to start modeling. First, we'll work through some logistic regressions, along with polynomial forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple logistic regression\n",
    "m_glm = LogisticRegression()\n",
    "m_glm.fit(X_train, y_train)\n",
    "df_train['y_pred_glm'] = m_glm.predict_proba(X_train)[:, 1]\n",
    "df_test['y_pred_glm'] = m_glm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"GLM Summary:\")\n",
    "print(f\"Coefficients: {m_glm.coef_[0]}\")\n",
    "print(f\"Intercept: {m_glm.intercept_[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avse_plot_classification(df_train, 'x_Age', 'y_out', 'y_pred_glm', 'exposures', 'exposures', 'freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial features\n",
    "degrees = [2, 3, 4, 9]\n",
    "glm_models = {}\n",
    "\n",
    "for degree in degrees:\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    X_test_poly = poly_features.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    df_train[f'y_pred_glm{degree}'] = model.predict_proba(X_train_poly)[:, 1]\n",
    "    df_test[f'y_pred_glm{degree}'] = model.predict_proba(X_test_poly)[:, 1]\n",
    "    \n",
    "    glm_models[f'glm{degree}'] = (model, poly_features)\n",
    "    print(f\"GLM{degree} fitted successfully\")\n",
    "\n",
    "print(\"All polynomial models fitted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize polynomial fits\n",
    "df2_pred = df2.copy()\n",
    "for degree in degrees:\n",
    "    model, poly_features = glm_models[f'glm{degree}']\n",
    "    X_age_poly = poly_features.transform(df2[['x_Age']])\n",
    "    df2_pred[f'f_pred_glm{degree}'] = model.predict_proba(X_age_poly)[:, 1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_true'], 'o-', color='red', label='True', linewidth=2)\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_actual'], 'o-', color='darkgreen', label='Actual', linewidth=2)\n",
    "\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "for i, degree in enumerate(degrees):\n",
    "    plt.plot(df2_pred['x_Age'], df2_pred[f'f_pred_glm{degree}'], \n",
    "             linestyle=linestyles[i], color='blue', label=f'GLM{degree}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('True vs Predicted Frequency by Age - Polynomial GLMs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM TIME\n",
    "\n",
    "Now, let's turn our attention to the GBM. Without knowing much else about GBM, let's use as many defaults as we can, and just send the data through the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GBM using scikit-learn's GradientBoostingClassifier\n",
    "m_gbm = GradientBoostingClassifier(random_state=31301)\n",
    "m_gbm.fit(X_train, y_train)\n",
    "\n",
    "df_train['y_pred_gbm'] = m_gbm.predict_proba(X_train)[:, 1]\n",
    "df_test['y_pred_gbm'] = m_gbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Add GBM predictions to age summary\n",
    "df2_pred['f_pred_gbm'] = m_gbm.predict_proba(df2[['x_Age']])[:, 1]\n",
    "\n",
    "print(\"GBM model fitted successfully!\")\n",
    "\n",
    "# Plot with GBM\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_true'], 'o-', color='red', label='True', linewidth=2)\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_actual'], 'o-', color='darkgreen', label='Actual', linewidth=2)\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_pred_glm2'], '--', color='blue', label='GLM2', linewidth=2)\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_pred_glm3'], '-.', color='blue', label='GLM3', linewidth=2)\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_pred_glm9'], ':', color='blue', label='GLM9', linewidth=2)\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_pred_gbm'], '-', color='purple', label='GBM', linewidth=3)\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Model Comparison: True vs Predicted Frequency by Age')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Congratulations! There's your first GBM (of today).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition from GBM to XGBoost\n",
    "\n",
    "Going forward, we are going to shift to XGBoost. For those interested, note GBM is a good function to get comfortable with, and is particularly useful for smaller scale modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to XGBoost format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# XGBoost parameters\n",
    "params = {\n",
    "    'eta': 0.3,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'seed': 31302\n",
    "}\n",
    "\n",
    "# Train XGBoost\n",
    "m_xgb = xgb.train(params, dtrain, num_boost_round=100, verbose_eval=False)\n",
    "\n",
    "df_train['y_pred_xgb'] = m_xgb.predict(dtrain)\n",
    "df_test['y_pred_xgb'] = m_xgb.predict(dtest)\n",
    "\n",
    "# Add to comparison plot\n",
    "df2_pred['f_pred_xgb'] = m_xgb.predict(xgb.DMatrix(df2[['x_Age']]))\n",
    "\n",
    "print(\"XGBoost model fitted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_true'], 'o-', color='red', label='True', linewidth=2)\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_actual'], 'o-', color='darkgreen', label='Actual', linewidth=2)\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_pred_glm2'], '--', color='blue', label='GLM2', linewidth=2)\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_pred_glm4'], '-.', color='blue', label='GLM4', linewidth=2)\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_pred_gbm'], '-', color='purple', label='GBM', linewidth=2)\n",
    "plt.plot(df2_pred['x_Age'], df2_pred['f_pred_xgb'], '-', color='orange', label='XGB', linewidth=3)\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Model Comparison with XGBoost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Results\n",
    "\n",
    "Lift Charts are a particular case of the Actual vs Expected plots. Instead of a variable, we line up the information based on the predicted responses. We then bin and check Actual vs Expected in the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deciles for lift analysis\n",
    "for model in ['y_pred_glm', 'y_pred_glm2', 'y_pred_glm4', 'y_pred_gbm', 'y_pred_xgb']:\n",
    "    df_test[f'{model}_decile'] = pd.qcut(df_test[model], 10, labels=False) + 1\n",
    "\n",
    "print(\"Deciles created for all models\")\n",
    "\n",
    "# Example lift chart for XGBoost\n",
    "avse_plot_classification(df_test, 'y_pred_xgb_decile', 'y_out', 'y_pred_xgb', 'exposures', 'exposures', 'freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "models = ['y_pred_glm', 'y_pred_glm2', 'y_pred_glm4', 'y_pred_gbm', 'y_pred_xgb']\n",
    "colors = ['red', 'darkgreen', 'blue', 'purple', 'orange']\n",
    "\n",
    "for model, color in zip(models, colors):\n",
    "    fpr, tpr, _ = roc_curve(y_test, df_test[model])\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=color, label=f'{model} (AUC = {auc_score:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison statistics\n",
    "model_names = ['GLM', 'GLM2', 'GLM4', 'GBM', 'XGB']\n",
    "model_preds_train = ['y_pred_glm', 'y_pred_glm2', 'y_pred_glm4', 'y_pred_gbm', 'y_pred_xgb']\n",
    "model_preds_test = ['y_pred_glm', 'y_pred_glm2', 'y_pred_glm4', 'y_pred_gbm', 'y_pred_xgb']\n",
    "\n",
    "comparison_df = pd.DataFrame()\n",
    "for name, train_pred, test_pred in zip(model_names, model_preds_train, model_preds_test):\n",
    "    stats = build_statline(y_train, df_train[train_pred], y_test, df_test[test_pred], name)\n",
    "    comparison_df = pd.concat([comparison_df, stats], ignore_index=True)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Complex Data Example\n",
    "\n",
    "This notebook demonstrates the core concepts from the R version. For more complex examples with multiple features, hyperparameter tuning, and advanced model evaluation, you can extend this notebook by:\n",
    "\n",
    "1. Adding more complex synthetic data with multiple features\n",
    "2. Implementing cross-validation for hyperparameter tuning\n",
    "3. Adding feature importance analysis\n",
    "4. Implementing more sophisticated model evaluation metrics\n",
    "\n",
    "The Python script version (`GCW_playcode_python.py`) contains the full implementation including these advanced features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This Python notebook successfully converts the key functionality from the R-based GCW_playcode notebook, demonstrating:\n",
    "\n",
    "- Data simulation with age-based risk patterns\n",
    "- Logistic regression with polynomial features\n",
    "- Gradient Boosting Machine (GBM) implementation\n",
    "- XGBoost modeling\n",
    "- Actual vs Expected plotting functions\n",
    "- Model evaluation with ROC curves and AUC metrics\n",
    "- Lift chart analysis\n",
    "\n",
    "**Next Steps:** To run this notebook, ensure you have all required packages installed:\n",
    "```bash\n",
    "pip install pandas numpy matplotlib seaborn scikit-learn xgboost\n",
    "```\n",
    "\n",
    "**Congratulations! You've successfully built your first GBM and XGBoost models in Python!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
